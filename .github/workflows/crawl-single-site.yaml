---
name: single url crawl

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'url to test'
        required: true
        default: 'https://www.sonarqube.ucl.ac.uk'
        type: string
      depth:
        description: 'depth of website to crawl'
        required: true
        default: 2
        type: int
      notify_email:
        description: 'notify email'
        required: false
        default: ''
        type: string

env:
  E_FDOMAINS:
  E_TDOMAINS:
  E_COOKIES:
  E_LISTNERS:
  E_TRACKERS:

run-name: "single url crawl ${{ inputs.url }} depth: ${{ inputs.depth }}"

jobs:
  crawl_site:
    runs-on: ubuntu-latest
    outputs:
      NUM_ENTRIES: ${{ steps.crawl_n_report.ENTRIES }}
      BIG_O: ${{ steps.crawl_n_report.BIG_O }}
      SITE_SIZE: ${{ steps.crawl_n_report.SIZE }}
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-go@v6
        with:
          go-version: '1.20'
      - run: |
          go version
          GO111MODULE=on go install github.com/jaeles-project/gospider@latest
          $HOME/go/bin/gospider --help
      - name: crawl_n_report
        env:
          URL: ${{ inputs.url }}
          DEPTH: ${{ inputs.depth }}
        run: |
          EX_FILES='.(jpg|jpeg|gif|css|tif|tiff|png|ttf|woff|woff2|ico|pdf|svg|js)'
          time $HOME/go/bin/gospider -d ${DEPTH} -c 10 --blacklist "${EX_FILES}" -s ${URL} |\
            tr -d '\r' |\
            grep -v -E '\[(form|subdomains|linkfinder|javascript)\]' crawl1.out |\
            grep -v -E 's/(.js|.css|.jpg|favicon.ico|.svg|.pdf)$//g' |\
            grep -v -E 's/(.js|.css)?ver=//g' | grep -v '\[code-40?\]'|\
            sed -e 's/^.* - //g' -e 's/http:/https:/g' -e 's/\/$//g' |\
            grep -v '^mailto:' | grep "^${URL}" |\
            grep -v wp-json/oembed |\
            sort -u > gospider.out

          export ENTRIES=$(wc -l gospider.out | cut -d ' ' -f1)
          echo "# entries: ${ENTRIES}"
          set -x
          echo "l(${ENTRIES})/l(10)" | bc -l 
          BIG_O=$(echo "l(${ENTRIES})/l(10)" | bc -l | sed 's/\.[0-9]*$//g')
          echo $BIG_O
          SIZE=L
          case "${BIG_O}" in
            "0")
              SIZE= ;;
            "1")
              SIZE=S ;;
            "2" | "3" )
              SIZE=M ;;
            *)
              SIZE=L ;;
          esac

          echo "NUM_ENTRIES=${ENTRIES}" >> "$GITHUB_OUTPUT"
          echo "BIG_O=${BIG_O}" >> "$GITHUB_OUTPUT"
          echo "SITE_SIZE=${SIZE}" >> "$GITHUB_OUTPUT"

          echo "### URL: [${URL}] crawl depth: [${DEPTH}]" >> $GITHUB_STEP_SUMMARY
          echo "### Crawled page count: [${ENTRIES}]" >> $GITHUB_STEP_SUMMARY
          echo "### Big O: [${BIG_O}] size is [${SIZE}]" >> $GITHUB_STEP_SUMMARY
  job2:
    runs-on: ubuntu-latest
    needs: [crawl_site]
    steps:
      - name: display vars
        env:
          NUM_ENTRIES: ${{ needs.crawl_n_report.NUM_ENTRIES }}
          BIG_O: ${{ needs.crawl_n_report.BIG_O }}
          SITE_SIZE: ${{ needs.crawl_n_report.SITE_SIZE }}
        run: |
          echo "### job2" >> $GITHUB_STEP_SUMMARY
          echo "### entries ${NUM_ENTRIES}" >> $GITHUB_STEP_SUMMARY
          echo "### big_o ${BIG_O} ${SITE_SIZE}" >> $GITHUB_STEP_SUMMARY

